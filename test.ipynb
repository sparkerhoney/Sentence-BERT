{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences1 = [\"\"\"{\n",
    "        \"Entity\": {\n",
    "            \"Entity.entity\": \"get the hang of\",\n",
    "            \"sub_type\": \"idiom\",\n",
    "            \"CEFR\": \"B1\",\n",
    "            \"core1\": \"Education and Knowledge\",\n",
    "            \"core2\": \"Emotions and Characteristics\",\n",
    "            \"core3\": \"Mind\",\n",
    "            \"source\": \"\",\n",
    "            \"usage_example\": \"After a few tries, she finally got the hang of playing the piano.\",\n",
    "            \"EXPLANATION\": \"to become familiar or skilled in doing something.\"\n",
    "        }\n",
    "    }\"\"\"]\n",
    "sentences2 = [\"\"\"{\n",
    "        \"FoundedEntity\": {\n",
    "            \"Entity.entity\": \"take off\",\n",
    "            \"sub_type\": \"phrasalVerb\",\n",
    "            \"usage_example\": \"\\\"take off\\\" means to remove something quickly, like a piece of clothing, but it can also mean to leave suddenly in the context of a plane \\\"taking off.\\\"\",\n",
    "            \"context_from_dialogue\": \"The tutor explains the dual meanings of \\\"take off\\\" in different contexts.\",\n",
    "            \"reason_of_score\": \"The phrasal verb is used appropriately, explaining its different meanings based on context, enhancing understanding.\"\n",
    "        },\n",
    "        \"Score\": 0.75\n",
    "    }\"\"\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input1 = tokenizer(sentences1, padding=True, truncation=True, return_tensors='pt')\n",
    "encoded_input2 = tokenizer(sentences2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output1 = model(**encoded_input1)\n",
    "    model_output2 = model(**encoded_input2)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings1 = mean_pooling(model_output1, encoded_input1['attention_mask'])\n",
    "sentence_embeddings2 = mean_pooling(model_output2, encoded_input2['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings1 = F.normalize(sentence_embeddings1, p=2, dim=1)\n",
    "sentence_embeddings2 = F.normalize(sentence_embeddings2, p=2, dim=1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = F.cosine_similarity(sentence_embeddings1, sentence_embeddings2)\n",
    "\n",
    "print(cosine_similarity.item())  # Print the cosine similarity as a Python float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
