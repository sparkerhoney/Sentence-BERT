# 센텐스버트(Sentence BERT, SBERT)

BERT로부터 문장 임베딩을 얻을 수 있는 센텐스버트(Sentence BERT, SBERT)에 대해서 다룹니다.


## 1. BERT의 문장 임베딩
BERT로부터 문장 벡터를 얻는 방법은 여러 가지가 있습니다. 예를 들어 'I love you'라는 문장이 입력되었을 때, 이 문장에 대한 벡터를 얻는 방법은 다음과 같습니다.

- 첫 번째 방법은 **[CLS] 토큰의 출력 벡터**를 문장 벡터로 간주하는 것입니다. [CLS] 토큰은 입력된 문장에 대한 총체적 표현으로 간주됩니다.

- 두 번째 방법은 BERT의 모든 출력 벡터들을 **평균내는 것**입니다. 이를 '평균 풀링(mean pooling)'이라고 합니다.

- 세 번째 방법은 **맥스 풀링(max pooling)**을 진행하여 벡터를 얻는 것입니다.

이러한 방법들은 문장 벡터가 가지는 의미에 차이를 줄 수 있습니다. 평균 풀링은 모든 단어의 의미를 반영하는 반면, 맥스 풀링은 중요한 단어의 의미를 반영합니다.

  
## 2. SBERT(센텐스버트, Sentence-BERT)
SBERT는 BERT의 문장 임베딩 성능을 개선한 모델로, BERT의 문장 임베딩을 응용하여 BERT를 파인 튜닝합니다.

### 1) 문장 쌍 분류 태스크로 파인 튜닝
SBERT는 문장 쌍 분류 태스크를 통해 학습되며, 대표적으로 NLI(Natural Language Inferencing) 문제를 풀게 됩니다.

- 예시: 문장 A와 문장 B가 주어졌을 때 두 문장 사이의 관계(수반, 모순, 중립)를 분류하는 문제입니다.

### 2) 문장 쌍 회귀 태스크로 파인 튜닝
또 다른 방법으로는 STS(Semantic Textual Similarity) 문제를 풀 수 있습니다. 이는 두 문장 사이의 의미적 유사성을 점수로 나타내는 문제입니다.

- 예시: 문장 A와 문장 B가 주어지고, 두 문장 사이의 유사도를 0~5 점 사이로 평가하는 문제입니다.

SBERT의 학습 구조는 문장 A와 B를 입력으로 받아 평균 풀링 또는 맥스 풀링을 통해 문장 임베딩 벡터를 얻고, 이를 통해 유사도를 계산하여 학습합니다.

한국어 버전의 STS 데이터셋인 KorSTS 데이터셋에 대한 정보는 다음 링크를 참고하십시오.
링크: [https://github.com/kakaobrain/KorNLUDatasets](https://github.com/kakaobrain/KorNLUDatasets)
