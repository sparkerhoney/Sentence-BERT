{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load Target Data\n",
    "with open('./data/target_data.json', 'r') as f:\n",
    "    target_data = json.load(f)\n",
    "\n",
    "# Load DB Embeddings Data\n",
    "db_embeddings_csv_path = './data/embeddings.csv'\n",
    "db_embeddings_df = pd.read_csv(db_embeddings_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>only_value_embedding</th>\n",
       "      <th>entire_json_embedding</th>\n",
       "      <th>field_name_and_value_embedding</th>\n",
       "      <th>multiple_appending_embedding</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ 7.68771693e-02  1.32324532e-01  1.16472028e-...</td>\n",
       "      <td>[-3.61508541e-02  9.70435068e-02  8.93538967e-...</td>\n",
       "      <td>[ 4.59796609e-03  7.10975705e-03  8.02040324e-...</td>\n",
       "      <td>[ 4.59796609e-03  7.10975705e-03  8.02040324e-...</td>\n",
       "      <td>{\"ENTITY\": \"a bear with a sore head\", \"SUB_TYP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ 3.13233323e-02  8.95459205e-02  4.66687903e-...</td>\n",
       "      <td>[-2.48112772e-02  7.21114725e-02  4.08676751e-...</td>\n",
       "      <td>[ 1.63082853e-02 -3.88042666e-02  4.28653099e-...</td>\n",
       "      <td>[ 1.63082853e-02 -3.88042666e-02  4.28653099e-...</td>\n",
       "      <td>{\"ENTITY\": \"a bird in the hand is worth two in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 1.04495004e-01  5.10501042e-02  3.66840698e-...</td>\n",
       "      <td>[-2.36943946e-04  2.06578132e-02  4.70502600e-...</td>\n",
       "      <td>[ 1.10081710e-01 -7.85264820e-02  1.07673407e-...</td>\n",
       "      <td>[ 1.10081710e-01 -7.85264820e-02  1.07673407e-...</td>\n",
       "      <td>{\"ENTITY\": \"a bit of a dark horse\", \"SUB_TYPE\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-2.74853148e-02  1.86494574e-01  3.63535099e-...</td>\n",
       "      <td>[-3.39237340e-02  8.06437507e-02  2.79941969e-...</td>\n",
       "      <td>[-2.32794527e-02 -3.63004431e-02 -1.31736277e-...</td>\n",
       "      <td>[-2.32794527e-02 -3.63004431e-02 -1.31736277e-...</td>\n",
       "      <td>{\"ENTITY\": \"a bitter pill to swallow\", \"SUB_TY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-1.61556199e-01  3.91429812e-01  1.83660403e-...</td>\n",
       "      <td>[-8.76663774e-02  1.61791265e-01  6.92320317e-...</td>\n",
       "      <td>[-1.03413567e-01  1.59799665e-01  8.75838771e-...</td>\n",
       "      <td>[-1.03413567e-01  1.59799665e-01  8.75838771e-...</td>\n",
       "      <td>{\"ENTITY\": \"a blessing in disguise\", \"SUB_TYPE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14835</th>\n",
       "      <td>[ 6.20404720e-01 -4.39046472e-01 -1.01780212e-...</td>\n",
       "      <td>[ 2.26157501e-01 -4.41266410e-02 -8.45891759e-...</td>\n",
       "      <td>[ 4.84701842e-01 -2.47120067e-01 -1.18561901e-...</td>\n",
       "      <td>[ 4.84701842e-01 -2.47120067e-01 -1.18561901e-...</td>\n",
       "      <td>{\"ENTITY\": \"zone\", \"SUB_TYPE\": \"vocabulary\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14836</th>\n",
       "      <td>[ 2.79354513e-01 -1.02612287e-01  1.42567396e-...</td>\n",
       "      <td>[ 9.58532393e-02 -8.09403788e-03  4.06629927e-...</td>\n",
       "      <td>[ 2.25922927e-01 -2.45514765e-01  1.70474127e-...</td>\n",
       "      <td>[ 2.25922927e-01 -2.45514765e-01  1.70474127e-...</td>\n",
       "      <td>{\"ENTITY\": \"zoo\", \"SUB_TYPE\": \"vocabulary\", \"C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14837</th>\n",
       "      <td>[ 1.68945730e-01  1.13200761e-01  3.98694314e-...</td>\n",
       "      <td>[ 3.15610766e-02  7.35686645e-02  1.26890028e-...</td>\n",
       "      <td>[ 1.10148706e-01 -1.53987378e-01  6.49411008e-...</td>\n",
       "      <td>[ 1.10148706e-01 -1.53987378e-01  6.49411008e-...</td>\n",
       "      <td>{\"ENTITY\": \"zoologist\", \"SUB_TYPE\": \"vocabular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14838</th>\n",
       "      <td>[ 1.64078087e-01 -1.57546729e-01  2.21433401e-...</td>\n",
       "      <td>[ 4.30875383e-02  5.05865403e-02  3.61127034e-...</td>\n",
       "      <td>[ 1.47771105e-01 -2.04341888e-01  9.56399515e-...</td>\n",
       "      <td>[ 1.47771105e-01 -2.04341888e-01  9.56399515e-...</td>\n",
       "      <td>{\"ENTITY\": \"zoology\", \"SUB_TYPE\": \"vocabulary\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14839</th>\n",
       "      <td>[ 1.90699846e-01 -2.56862789e-01 -7.76219368e-...</td>\n",
       "      <td>[ 6.33434579e-02 -5.92913963e-02  6.54196320e-...</td>\n",
       "      <td>[ 1.65786326e-01 -3.48040313e-01 -5.60533777e-...</td>\n",
       "      <td>[ 1.65786326e-01 -3.48040313e-01 -5.60533777e-...</td>\n",
       "      <td>{\"ENTITY\": \"zoom\", \"SUB_TYPE\": \"vocabulary\", \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14840 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    only_value_embedding  \\\n",
       "0      [ 7.68771693e-02  1.32324532e-01  1.16472028e-...   \n",
       "1      [ 3.13233323e-02  8.95459205e-02  4.66687903e-...   \n",
       "2      [ 1.04495004e-01  5.10501042e-02  3.66840698e-...   \n",
       "3      [-2.74853148e-02  1.86494574e-01  3.63535099e-...   \n",
       "4      [-1.61556199e-01  3.91429812e-01  1.83660403e-...   \n",
       "...                                                  ...   \n",
       "14835  [ 6.20404720e-01 -4.39046472e-01 -1.01780212e-...   \n",
       "14836  [ 2.79354513e-01 -1.02612287e-01  1.42567396e-...   \n",
       "14837  [ 1.68945730e-01  1.13200761e-01  3.98694314e-...   \n",
       "14838  [ 1.64078087e-01 -1.57546729e-01  2.21433401e-...   \n",
       "14839  [ 1.90699846e-01 -2.56862789e-01 -7.76219368e-...   \n",
       "\n",
       "                                   entire_json_embedding  \\\n",
       "0      [-3.61508541e-02  9.70435068e-02  8.93538967e-...   \n",
       "1      [-2.48112772e-02  7.21114725e-02  4.08676751e-...   \n",
       "2      [-2.36943946e-04  2.06578132e-02  4.70502600e-...   \n",
       "3      [-3.39237340e-02  8.06437507e-02  2.79941969e-...   \n",
       "4      [-8.76663774e-02  1.61791265e-01  6.92320317e-...   \n",
       "...                                                  ...   \n",
       "14835  [ 2.26157501e-01 -4.41266410e-02 -8.45891759e-...   \n",
       "14836  [ 9.58532393e-02 -8.09403788e-03  4.06629927e-...   \n",
       "14837  [ 3.15610766e-02  7.35686645e-02  1.26890028e-...   \n",
       "14838  [ 4.30875383e-02  5.05865403e-02  3.61127034e-...   \n",
       "14839  [ 6.33434579e-02 -5.92913963e-02  6.54196320e-...   \n",
       "\n",
       "                          field_name_and_value_embedding  \\\n",
       "0      [ 4.59796609e-03  7.10975705e-03  8.02040324e-...   \n",
       "1      [ 1.63082853e-02 -3.88042666e-02  4.28653099e-...   \n",
       "2      [ 1.10081710e-01 -7.85264820e-02  1.07673407e-...   \n",
       "3      [-2.32794527e-02 -3.63004431e-02 -1.31736277e-...   \n",
       "4      [-1.03413567e-01  1.59799665e-01  8.75838771e-...   \n",
       "...                                                  ...   \n",
       "14835  [ 4.84701842e-01 -2.47120067e-01 -1.18561901e-...   \n",
       "14836  [ 2.25922927e-01 -2.45514765e-01  1.70474127e-...   \n",
       "14837  [ 1.10148706e-01 -1.53987378e-01  6.49411008e-...   \n",
       "14838  [ 1.47771105e-01 -2.04341888e-01  9.56399515e-...   \n",
       "14839  [ 1.65786326e-01 -3.48040313e-01 -5.60533777e-...   \n",
       "\n",
       "                            multiple_appending_embedding  \\\n",
       "0      [ 4.59796609e-03  7.10975705e-03  8.02040324e-...   \n",
       "1      [ 1.63082853e-02 -3.88042666e-02  4.28653099e-...   \n",
       "2      [ 1.10081710e-01 -7.85264820e-02  1.07673407e-...   \n",
       "3      [-2.32794527e-02 -3.63004431e-02 -1.31736277e-...   \n",
       "4      [-1.03413567e-01  1.59799665e-01  8.75838771e-...   \n",
       "...                                                  ...   \n",
       "14835  [ 4.84701842e-01 -2.47120067e-01 -1.18561901e-...   \n",
       "14836  [ 2.25922927e-01 -2.45514765e-01  1.70474127e-...   \n",
       "14837  [ 1.10148706e-01 -1.53987378e-01  6.49411008e-...   \n",
       "14838  [ 1.47771105e-01 -2.04341888e-01  9.56399515e-...   \n",
       "14839  [ 1.65786326e-01 -3.48040313e-01 -5.60533777e-...   \n",
       "\n",
       "                                                  entity  \n",
       "0      {\"ENTITY\": \"a bear with a sore head\", \"SUB_TYP...  \n",
       "1      {\"ENTITY\": \"a bird in the hand is worth two in...  \n",
       "2      {\"ENTITY\": \"a bit of a dark horse\", \"SUB_TYPE\"...  \n",
       "3      {\"ENTITY\": \"a bitter pill to swallow\", \"SUB_TY...  \n",
       "4      {\"ENTITY\": \"a blessing in disguise\", \"SUB_TYPE...  \n",
       "...                                                  ...  \n",
       "14835  {\"ENTITY\": \"zone\", \"SUB_TYPE\": \"vocabulary\", \"...  \n",
       "14836  {\"ENTITY\": \"zoo\", \"SUB_TYPE\": \"vocabulary\", \"C...  \n",
       "14837  {\"ENTITY\": \"zoologist\", \"SUB_TYPE\": \"vocabular...  \n",
       "14838  {\"ENTITY\": \"zoology\", \"SUB_TYPE\": \"vocabulary\"...  \n",
       "14839  {\"ENTITY\": \"zoom\", \"SUB_TYPE\": \"vocabulary\", \"...  \n",
       "\n",
       "[14840 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m top_matches\n\u001b[1;32m     63\u001b[0m \u001b[39m# Process Target Data and Calculate Top Matches\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m top_matches \u001b[39m=\u001b[39m embed_and_compare(target_data, db_embeddings_df)\n\u001b[1;32m     66\u001b[0m \u001b[39m# Print top matches for inspection\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m method, matches \u001b[39min\u001b[39;00m top_matches\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[12], line 57\u001b[0m, in \u001b[0;36membed_and_compare\u001b[0;34m(target_data, db_embeddings_df)\u001b[0m\n\u001b[1;32m     55\u001b[0m target_embedding \u001b[39m=\u001b[39m mean_pooling(model_output, encoded_input[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     56\u001b[0m \u001b[39m# Compute cosine similarities\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m cosine_scores \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcosine_similarity(target_embedding, db_embeddings, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Get top 3 indices\u001b[39;00m\n\u001b[1;32m     59\u001b[0m top_indices \u001b[39m=\u001b[39m cosine_scores\u001b[39m.\u001b[39mtopk(\u001b[39m3\u001b[39m, largest\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mindices\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load Target Data\n",
    "with open('./data/target_data.json', 'r') as f:\n",
    "    target_data = json.load(f)\n",
    "\n",
    "# Load DB Embeddings Data\n",
    "db_embeddings_csv_path = './data/embeddings.csv'\n",
    "db_embeddings_df = pd.read_csv(db_embeddings_csv_path)\n",
    "\n",
    "# Function to convert string representations of lists into actual lists\n",
    "def string_to_list(string_list):\n",
    "    try:\n",
    "        return ast.literal_eval(string_list)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Function to get embeddings as tensors from the DataFrame\n",
    "def get_embeddings_tensor(df, column):\n",
    "    embeddings = []\n",
    "    for item in df[column]:\n",
    "        item_list = string_to_list(item) if isinstance(item, str) else item\n",
    "        if item_list is not None:\n",
    "            embeddings.append(item_list)\n",
    "    return torch.tensor(embeddings) if embeddings else torch.tensor([])\n",
    "\n",
    "# Define mean pooling function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Embedding and Cosine Similarity Calculation\n",
    "def embed_and_compare(target_data, db_embeddings_df):\n",
    "    top_matches = {}\n",
    "    for method in ['only_value', 'entire_json', 'field_name_and_value', 'multiple_appending']:\n",
    "        db_embeddings = get_embeddings_tensor(db_embeddings_df, f'{method}_embedding')\n",
    "        top_matches[method] = []\n",
    "        # Compute embedding for the target data\n",
    "        for target_entry in target_data:\n",
    "            text_data = target_entry['FoundedEntity']['Entity.entity']\n",
    "            encoded_input = tokenizer(text_data, padding=True, truncation=True, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "            target_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "            # Compute cosine similarities\n",
    "            cosine_scores = F.cosine_similarity(target_embedding, db_embeddings, dim=1)\n",
    "            # Get top 3 indices\n",
    "            top_indices = cosine_scores.topk(3, largest=True).indices\n",
    "            top_matches[method] += db_embeddings_df.iloc[top_indices].to_dict('records')\n",
    "    return top_matches\n",
    "\n",
    "# Process Target Data and Calculate Top Matches\n",
    "top_matches = embed_and_compare(target_data, db_embeddings_df)\n",
    "\n",
    "# Print top matches for inspection\n",
    "for method, matches in top_matches.items():\n",
    "    print(f\"{method} Top Matches:\")\n",
    "    for match in matches:\n",
    "        print(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b55ca2bf9d47808b65b3eff55a1092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Methods:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m top_matches\n\u001b[1;32m     68\u001b[0m \u001b[39m# Process Target Data and Calculate Top Matches\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m top_matches \u001b[39m=\u001b[39m embed_and_compare(target_data, db_embeddings_df)\n\u001b[1;32m     71\u001b[0m \u001b[39m# Print top matches for inspection\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m method, matches \u001b[39min\u001b[39;00m top_matches\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[17], line 49\u001b[0m, in \u001b[0;36membed_and_compare\u001b[0;34m(target_entries, db_embeddings_df)\u001b[0m\n\u001b[1;32m     45\u001b[0m top_matches \u001b[39m=\u001b[39m {}\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m method \u001b[39min\u001b[39;00m tqdm([\u001b[39m'\u001b[39m\u001b[39monly_value\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mentire_json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfield_name_and_value\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmultiple_appending\u001b[39m\u001b[39m'\u001b[39m], desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMethods\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[39m# Convert stringified embeddings back to lists\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     db_embeddings_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m---> 49\u001b[0m         [json\u001b[39m.\u001b[39mloads(embedding) \u001b[39mfor\u001b[39;00m embedding \u001b[39min\u001b[39;00m db_embeddings_df[method \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_embedding\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m tqdm(target_entries, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEmbedding target data\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     53\u001b[0m         text_data \u001b[39m=\u001b[39m entry[\u001b[39m'\u001b[39m\u001b[39mFoundedEntity\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mEntity.entity\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[17], line 49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m top_matches \u001b[39m=\u001b[39m {}\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m method \u001b[39min\u001b[39;00m tqdm([\u001b[39m'\u001b[39m\u001b[39monly_value\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mentire_json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfield_name_and_value\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmultiple_appending\u001b[39m\u001b[39m'\u001b[39m], desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMethods\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[39m# Convert stringified embeddings back to lists\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     db_embeddings_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m---> 49\u001b[0m         [json\u001b[39m.\u001b[39;49mloads(embedding) \u001b[39mfor\u001b[39;00m embedding \u001b[39min\u001b[39;00m db_embeddings_df[method \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_embedding\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m tqdm(target_entries, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEmbedding target data\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     53\u001b[0m         text_data \u001b[39m=\u001b[39m entry[\u001b[39m'\u001b[39m\u001b[39mFoundedEntity\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mEntity.entity\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/sbert/lib/python3.8/json/__init__.py:341\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(s, (\u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m)):\n\u001b[0;32m--> 341\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    342\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kw:\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load Target Data\n",
    "with open('./data/target_data.json', 'r') as f:\n",
    "    target_data = json.load(f)\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load DB Embeddings Data\n",
    "db_embeddings_csv_path = './data/embeddings.csv'\n",
    "db_embeddings_df = pd.read_csv(db_embeddings_csv_path)\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "def string_to_list(string_list):\n",
    "    try:\n",
    "        return ast.literal_eval(string_list)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None  # Or return an empty list if you prefer []\n",
    "\n",
    "# Update DataFrame columns to contain actual lists instead of strings\n",
    "for col in ['only_value_embedding', 'entire_json_embedding', 'field_name_and_value_embedding', 'multiple_appending_embedding']:\n",
    "    db_embeddings_df[col] = db_embeddings_df[col].apply(string_to_list)\n",
    "\n",
    "# Now you can proceed with the embeddings since they are proper lists\n",
    "\n",
    "\n",
    "# Define mean pooling function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    return sum_embeddings / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Embedding and Cosine Similarity Calculation\n",
    "def embed_and_compare(target_entries, db_embeddings_df):\n",
    "    top_matches = {}\n",
    "    for method in tqdm(['only_value', 'entire_json', 'field_name_and_value', 'multiple_appending'], desc='Methods'):\n",
    "        # Convert stringified embeddings back to lists\n",
    "        db_embeddings_tensor = torch.tensor(\n",
    "            [json.loads(embedding) for embedding in db_embeddings_df[method + '_embedding']]\n",
    "        )\n",
    "        \n",
    "        for entry in tqdm(target_entries, desc='Embedding target data'):\n",
    "            text_data = entry['FoundedEntity']['Entity.entity']\n",
    "            encoded_input = tokenizer(text_data, padding=True, truncation=True, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "            target_embedding = mean_pooling(model_output, encoded_input['attention_mask']).unsqueeze(0)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cosine_scores = F.cosine_similarity(target_embedding, db_embeddings_tensor, dim=1)\n",
    "            # Get top 3 matches\n",
    "            top_scores, top_indices = cosine_scores.topk(3, largest=True)\n",
    "            # Add matches to the results\n",
    "            top_matches[method] = db_embeddings_df.iloc[top_indices]['entity'].tolist()\n",
    "\n",
    "    return top_matches\n",
    "\n",
    "# Process Target Data and Calculate Top Matches\n",
    "top_matches = embed_and_compare(target_data, db_embeddings_df)\n",
    "\n",
    "# Print top matches for inspection\n",
    "for method, matches in top_matches.items():\n",
    "    print(f\"{method} Top Matches:\")\n",
    "    print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>only_value_embedding</th>\n",
       "      <th>entire_json_embedding</th>\n",
       "      <th>field_name_and_value_embedding</th>\n",
       "      <th>multiple_appending_embedding</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"a bear with a sore head\", \"SUB_TYP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"a bird in the hand is worth two in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"a bit of a dark horse\", \"SUB_TYPE\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"a bitter pill to swallow\", \"SUB_TY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"a blessing in disguise\", \"SUB_TYPE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14835</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"zone\", \"SUB_TYPE\": \"vocabulary\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14836</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"zoo\", \"SUB_TYPE\": \"vocabulary\", \"C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14837</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"zoologist\", \"SUB_TYPE\": \"vocabular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14838</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"zoology\", \"SUB_TYPE\": \"vocabulary\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14839</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\"ENTITY\": \"zoom\", \"SUB_TYPE\": \"vocabulary\", \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14840 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      only_value_embedding entire_json_embedding  \\\n",
       "0                     None                  None   \n",
       "1                     None                  None   \n",
       "2                     None                  None   \n",
       "3                     None                  None   \n",
       "4                     None                  None   \n",
       "...                    ...                   ...   \n",
       "14835                 None                  None   \n",
       "14836                 None                  None   \n",
       "14837                 None                  None   \n",
       "14838                 None                  None   \n",
       "14839                 None                  None   \n",
       "\n",
       "      field_name_and_value_embedding multiple_appending_embedding  \\\n",
       "0                               None                         None   \n",
       "1                               None                         None   \n",
       "2                               None                         None   \n",
       "3                               None                         None   \n",
       "4                               None                         None   \n",
       "...                              ...                          ...   \n",
       "14835                           None                         None   \n",
       "14836                           None                         None   \n",
       "14837                           None                         None   \n",
       "14838                           None                         None   \n",
       "14839                           None                         None   \n",
       "\n",
       "                                                  entity  \n",
       "0      {\"ENTITY\": \"a bear with a sore head\", \"SUB_TYP...  \n",
       "1      {\"ENTITY\": \"a bird in the hand is worth two in...  \n",
       "2      {\"ENTITY\": \"a bit of a dark horse\", \"SUB_TYPE\"...  \n",
       "3      {\"ENTITY\": \"a bitter pill to swallow\", \"SUB_TY...  \n",
       "4      {\"ENTITY\": \"a blessing in disguise\", \"SUB_TYPE...  \n",
       "...                                                  ...  \n",
       "14835  {\"ENTITY\": \"zone\", \"SUB_TYPE\": \"vocabulary\", \"...  \n",
       "14836  {\"ENTITY\": \"zoo\", \"SUB_TYPE\": \"vocabulary\", \"C...  \n",
       "14837  {\"ENTITY\": \"zoologist\", \"SUB_TYPE\": \"vocabular...  \n",
       "14838  {\"ENTITY\": \"zoology\", \"SUB_TYPE\": \"vocabulary\"...  \n",
       "14839  {\"ENTITY\": \"zoom\", \"SUB_TYPE\": \"vocabulary\", \"...  \n",
       "\n",
       "[14840 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dac0c15cb8a482da996182d47e7dca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding target data for only_value:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m top_matches\n\u001b[1;32m     65\u001b[0m \u001b[39m# Process Target Data and Calculate Top Matches\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m top_matches \u001b[39m=\u001b[39m embed_and_compare(target_data, db_embeddings_df)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Print top matches for inspection\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m method, matches \u001b[39min\u001b[39;00m top_matches\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[23], line 58\u001b[0m, in \u001b[0;36membed_and_compare\u001b[0;34m(target_entries, db_embeddings_df)\u001b[0m\n\u001b[1;32m     55\u001b[0m target_embedding \u001b[39m=\u001b[39m mean_pooling(model_output, encoded_input[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Calculate cosine similarity\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m cosine_scores \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcosine_similarity(target_embedding, db_embeddings_tensor, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     59\u001b[0m \u001b[39m# Get top 3 matches\u001b[39;00m\n\u001b[1;32m     60\u001b[0m top_scores, top_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(cosine_scores, \u001b[39m3\u001b[39m, largest\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the embeddings CSV\n",
    "db_embeddings_csv_path = './data/embeddings.csv'  # Update with the correct path\n",
    "db_embeddings_df = pd.read_csv(db_embeddings_csv_path)\n",
    "\n",
    "# Load the target JSON data\n",
    "target_data_json_path = './data/target_data.json'  # Update with the correct path\n",
    "with open(target_data_json_path, 'r') as f:\n",
    "    target_data = json.load(f)\n",
    "\n",
    "# Initialize the tokenizer and model from HuggingFace Transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Define the mean pooling function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Convert embedding strings to actual lists\n",
    "def string_to_list(string_list):\n",
    "    try:\n",
    "        return ast.literal_eval(string_list)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Update DataFrame columns to contain actual lists instead of strings\n",
    "for col in db_embeddings_df.columns:\n",
    "    if col.endswith('_embedding'):\n",
    "        db_embeddings_df[col] = db_embeddings_df[col].apply(string_to_list)\n",
    "\n",
    "# Embedding and Cosine Similarity Calculation\n",
    "def embed_and_compare(target_entries, db_embeddings_df):\n",
    "    top_matches = {}\n",
    "    methods = ['only_value', 'entire_json', 'field_name_and_value', 'multiple_appending']\n",
    "    embeddings_columns = [f\"{method}_embedding\" for method in methods]\n",
    "\n",
    "    for method, column in zip(methods, embeddings_columns):\n",
    "        db_embeddings_tensor = torch.tensor(list(db_embeddings_df[column].dropna()), dtype=torch.float32)\n",
    "        top_matches[method] = []\n",
    "\n",
    "        for entry in tqdm(target_entries, desc=f'Embedding target data for {method}'):\n",
    "            text_data = entry['FoundedEntity']['Entity.entity']\n",
    "            encoded_input = tokenizer(text_data, padding=True, truncation=True, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "            target_embedding = mean_pooling(model_output, encoded_input['attention_mask']).unsqueeze(0)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cosine_scores = F.cosine_similarity(target_embedding, db_embeddings_tensor, dim=1)\n",
    "            # Get top 3 matches\n",
    "            top_scores, top_indices = torch.topk(cosine_scores, 3, largest=True)\n",
    "            top_matches[method].extend(db_embeddings_df.iloc[top_indices]['entity'].tolist())\n",
    "\n",
    "    return top_matches\n",
    "\n",
    "# Process Target Data and Calculate Top Matches\n",
    "top_matches = embed_and_compare(target_data, db_embeddings_df)\n",
    "\n",
    "# Print top matches for inspection\n",
    "for method, matches in top_matches.items():\n",
    "    print(f\"{method} Top Matches:\")\n",
    "    print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m col\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m_embedding\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m         df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39;49mapply(string_to_tensor)\n\u001b[1;32m     50\u001b[0m \u001b[39m# Embedding the target data\u001b[39;00m\n\u001b[1;32m     51\u001b[0m encoded_target \u001b[39m=\u001b[39m tokenizer(target_data, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sbert/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/sbert/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/sbert/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/sbert/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mstring_to_tensor\u001b[0;34m(embedding_str)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstring_to_tensor\u001b[39m(embedding_str):\n\u001b[0;32m---> 42\u001b[0m     embedding_list \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m(embedding_str)\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(embedding_list, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = './data/embeddings.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the target data\n",
    "with open('./data/target_data.json', 'r') as f:\n",
    "    target_data = json.load(f)\n",
    "\n",
    "# Initialize the tokenizer and model from HuggingFace Transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Convert string embeddings in dataframe to list of floats\n",
    "def convert_embedding_string_to_list(embedding_str):\n",
    "    # Handle empty strings or NaN values\n",
    "    if pd.isna(embedding_str):\n",
    "        return []\n",
    "    # Convert the string to a list of floats\n",
    "    embedding_list = json.loads(embedding_str)\n",
    "    return embedding_list\n",
    "\n",
    "# Apply the conversion function to embedding columns\n",
    "embedding_cols = ['only_value_embedding', 'entire_json_embedding', 'field_name_and_value_embedding', 'multiple_appending_embedding']\n",
    "for col in embedding_cols:\n",
    "    df[col] = df[col].apply(convert_embedding_string_to_list)\n",
    "    \n",
    "# Function to calculate the cosine similarity between two tensors\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    tensor1_norm = F.normalize(tensor1, p=2, dim=1)\n",
    "    tensor2_norm = F.normalize(tensor2, p=2, dim=1)\n",
    "    return torch.mm(tensor1_norm, tensor2_norm.transpose(0, 1))\n",
    "\n",
    "# Function to convert string embeddings in dataframe to tensor\n",
    "def string_to_tensor(embedding_str):\n",
    "    embedding_list = eval(embedding_str)\n",
    "    return torch.tensor(embedding_list, dtype=torch.float32)\n",
    "\n",
    "# Convert the string embeddings to tensors\n",
    "for col in df.columns:\n",
    "    if col.endswith('_embedding'):\n",
    "        df[col] = df[col].apply(string_to_tensor)\n",
    "\n",
    "# Embedding the target data\n",
    "encoded_target = tokenizer(target_data, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_target)\n",
    "target_embeddings = model_output[0]  # Assuming mean pooling is already applied during model output\n",
    "\n",
    "# Calculate cosine similarity for each embedding type\n",
    "results = {}\n",
    "for method in ['only_value', 'entire_json', 'field_name_and_value', 'multiple_appending']:\n",
    "    embedding_col = f'{method}_embedding'\n",
    "    db_embeddings_tensor = torch.stack(df[embedding_col].tolist())\n",
    "    cos_sim = cosine_similarity(target_embeddings, db_embeddings_tensor)\n",
    "    top_similarities, top_indices = torch.topk(cos_sim, 3, largest=True)\n",
    "    results[method] = {\n",
    "        'similarities': top_similarities.numpy().tolist(),\n",
    "        'indices': top_indices.numpy().tolist()\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
